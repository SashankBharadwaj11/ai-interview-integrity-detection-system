# ğŸ¥ğŸ” AI Interview Integrity Detection System  
*Offline Audio + Video Monitoring for Exams & Interviews*

This project performs **offline integrity analysis** on pre-recorded **video + audio** to identify suspicious behavior during exams or interviews.

It generates a detailed **Session Integrity Report** including:
- Face presence tracking  
- Eye & gaze direction  
- Blink rate / EAR  
- Object detection  
- Multi-face presence  
- Speaker consistency scoring  
- Timeline graphs  
- Combined integrity score  

All processing runs **locally** â€” no uploads or data sharing.

---

# ğŸš€ Features

## ğŸï¸ Video Analysis
- Face detection (MTCNN â€“ facenet-pytorch)  
- Eye tracking & facial landmarks (MediaPipe FaceMesh)  
- Gaze direction classification (left/right/up/down/center)  
- Blink detection via EAR  
- Face-missing alerts  
- Multi-face detection  

## ğŸ“¦ Object Detection
Model: **YOLOv8-Nano (Ultralytics)**  
Detects:
- ğŸ“± Mobile phones  
- ğŸ“š Books / notes  
- ğŸ“ Papers  
- (Extendable via `object_detection.py`)  

## ğŸ”Š Audio Integrity Analysis
Powered by **WavLM-Base+ (HuggingFace)**

Pipeline:
1. Extract audio (ffmpeg)  
2. Split audio (2â€“3 sec chunks)  
3. Compute embeddings  
4. Compare cosine similarity  
5. Detect speaker change  

Outputs:
- Average similarity  
- Minimum similarity  
- Speaker change flag  
- Audio integrity score (0â€“100)  

## ğŸ“‘ Session Integrity Report
- Alerts summary  
- Object detection activity  
- Timeline graphs (Chart.js)  
- Speaker consistency graph  
- Combined score  
- Auto-saved JSON reports  
- Displayed via Flask dashboard  

---

# ğŸ§± Project Architecture

ai-interview-integrity-detection-system/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ dashboard/ # Flask dashboard UI
â”‚ â”‚ â”œâ”€â”€ app.py
â”‚ â”‚ â””â”€â”€ templates/ # HTML templates
â”‚ â”‚ â”œâ”€â”€ dashboard.html
â”‚ â”‚ â”œâ”€â”€ upload.html
â”‚ â”‚ â””â”€â”€ session_report.html
â”‚ â”‚
â”‚ â”œâ”€â”€ detection/ # Video detection modules
â”‚ â”‚ â”œâ”€â”€ face_detection.py
â”‚ â”‚ â”œâ”€â”€ eye_tracking.py
â”‚ â”‚ â”œâ”€â”€ object_detection.py
â”‚ â”‚ â””â”€â”€ multi_face.py
â”‚ â”‚
â”‚ â”œâ”€â”€ audio/ # Audio processing modules
â”‚ â”‚ â”œâ”€â”€ speaker_consistency.py
â”‚ â”‚ â””â”€â”€ utils_audio.py
â”‚ â”‚
â”‚ â”œâ”€â”€ analysis/ # Scoring + reporting logic
â”‚ â”‚ â”œâ”€â”€ scoring.py
â”‚ â”‚ â””â”€â”€ report_generator.py
â”‚ â”‚
â”‚ â”œâ”€â”€ utils/ # Utility helpers
â”‚ â”‚ â”œâ”€â”€ logging.py
â”‚ â”‚ â”œâ”€â”€ screenshot_utils.py
â”‚ â”‚ â””â”€â”€ timer.py
â”‚ â”‚
â”‚ â”œâ”€â”€ offline_processor.py # Full offline pipeline
â”‚ â””â”€â”€ config.yaml # Detection configuration
â”‚
â”œâ”€â”€ uploads/ # Uploaded video/audio
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ sessions/ # JSON session reports
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

# âš™ï¸ Installation

## 1ï¸âƒ£ Create Conda Environment
```bash
conda create -n interview310 python=3.10
conda activate interview310
2ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
3ï¸âƒ£ Install ffmpeg (Required for audio extraction)
macOS:
bash
Copy code
brew install ffmpeg
ğŸ§ª Running the Application
Start the Flask dashboard:

bash
Copy code
python -m src.dashboard.app
Now open:

cpp
Copy code
http://127.0.0.1:5000
ğŸ“¤ How to Use the System
Upload video recording

Upload audio recording

Click Analyze Recording

Processing time: 1â€“2 min per 15 min video

View Session Integrity Report

ğŸ“Š Scoring System
ğŸï¸ Video Integrity Score (0â€“100)
Penalties for:

Face missing

Looking away (L/R/U/D)

Excessive eye movement

Multi-face detection

Forbidden objects

ğŸ”Š Audio Integrity Score (0â€“100)
Based on WavLM similarity:

High similarity = same speaker

Low similarity = possible switch

speaker_change_flag = True â†’ penalty applied

â­ Combined Overall Score
Formula:

python
Copy code
overall_score = 0.7 * video_score + 0.3 * audio_score
ğŸ§  Models Used
Task	Model	Framework
Face Detection	MTCNN	facenet-pytorch
Eye Tracking	FaceMesh	MediaPipe
Object Detection	YOLOv8n	Ultralytics
Speaker Embeddings	WavLM-Base+	HuggingFace Transformers
Audio Extraction	ffmpeg	subprocess

ğŸ§© Configuration (config.yaml)
Below is a sample config:

yaml
Copy code
detection:
  face:
    detection_interval: 5
    min_confidence: 0.8

  eyes:
    gaze_threshold: 2
    blink_threshold: 0.3

  objects:
    min_confidence: 0.65
    max_fps: 5

audio_monitoring:
  sample_rate: 16000
Modify these to customize system behavior.

ğŸ§¼ Code Quality Improvements
Unified scoring pipeline

Robust JSON schema

Cleaner Jinja templates

YOLO inference optimization

Isolated audio subsystem

Environment fixes

Removed duplicate envs + conflicts

Support for command-line offline processing


ğŸ“¬ Future Enhancements
Real-time webcam detection

Emotion detection

OCR for desk notes

Multi-speaker diarization

Deepfake voice detection

GPU FastAPI deployment

