ğŸ¥ğŸ” AI Interview Integrity Detection System
Offline Audio + Video Analysis for Exam/Interview Monitoring

This project is an AI-driven integrity analysis system that processes offline video + audio recordings to detect suspicious behavior during online interviews or exam sessions.

It generates a detailed Session Integrity Report with:

ğŸ§‘â€ğŸ’» Face presence tracking

ğŸ‘€ Eye & gaze direction analysis

ğŸ§­ Blink/EAR tracking

ğŸ§â€â™‚ï¸ Multi-face detection

ğŸ“±ğŸ“š Forbidden object detection

ğŸ”Š Speaker-consistency scoring using WavLM

ğŸ“Š Timeline visualizations (Chart.js)

â­ Overall integrity score (0â€“100)

Everything runs locally on the userâ€™s machine â€” no data leaves the system.

ğŸš€ Key Features
ğŸï¸ Video Analysis

Face detection (MTCNN, facenet-pytorch)

Eye tracking (MediaPipe FaceMesh)

Gaze classification: left, right, center, up, down

EAR-based blink detection

Face-missing event alerts

Multi-face detection (detect extra persons)

FPS-aware optimizations

ğŸ“¦ Object Detection

Model: YOLOv8-Nano (Ultralytics)
Detects:

ğŸ“± Mobile phones

ğŸ“š Books/notes

ğŸ“ Paper sheets

(Easily extendable via object_detection.py)

ğŸ”Š Audio Integrity Analysis

Powered by WavLM-Base+ (HuggingFace).

Pipeline:

Extract audio (ffmpeg)

Split into chunks (2â€“3 seconds)

Generate embeddings per chunk

Compute:

Average similarity

Minimum chunk similarity

Speaker change probability

Final Speaker Consistency Score (0â€“100)

ğŸ“‘ Session Integrity Report

Generated via report_generator.py.

Includes:

Alerts summary

Object detection hits

Video activity timeline

Speaker consistency graph

Weighted combined integrity score

Auto-saved JSON at logs/sessions/

Rendered with Flask templates

ğŸ§± Project Architecture
ai-interview-integrity-detection-system/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dashboard/                  # Flask Web UI
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚       â”œâ”€â”€ dashboard.html
â”‚   â”‚       â”œâ”€â”€ upload.html
â”‚   â”‚       â””â”€â”€ session_report.html
â”‚   â”‚
â”‚   â”œâ”€â”€ detection/                  # Video detection modules
â”‚   â”‚   â”œâ”€â”€ face_detection.py
â”‚   â”‚   â”œâ”€â”€ eye_tracking.py
â”‚   â”‚   â”œâ”€â”€ object_detection.py
â”‚   â”‚   â””â”€â”€ multi_face.py
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                      # Audio pipeline
â”‚   â”‚   â”œâ”€â”€ speaker_consistency.py
â”‚   â”‚   â””â”€â”€ utils_audio.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                   # Scoring logic
â”‚   â”‚   â”œâ”€â”€ scoring.py
â”‚   â”‚   â””â”€â”€ report_generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”œâ”€â”€ screenshot_utils.py
â”‚   â”‚   â””â”€â”€ timer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ offline_processor.py        # Main offline pipeline
â”‚   â””â”€â”€ config.yaml                 # Detection parameters
â”‚
â”œâ”€â”€ uploads/                         # User-uploaded video/audio
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ sessions/                    # JSON reports
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation
1ï¸âƒ£ Create Conda Environment
conda create -n interview310 python=3.10
conda activate interview310

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install ffmpeg (Required)

macOS:

brew install ffmpeg

ğŸ§ª Running the Application

Start the Flask dashboard:

python -m src.dashboard.app


Then visit:

http://127.0.0.1:5000

ğŸ“¤ How to Use the System

Upload pre-recorded video

Upload associated audio file (recommended same duration)

Click Analyze Recording

Processing takes about 1â€“2 minutes per 15-minute video

View the full Session Integrity Report

ğŸ“Š Scoring System
ğŸï¸ Video Integrity Score (0â€“100)

Penalizes:

Face missing

Gaze away (L/R/U/D)

Excessive eye movement

Multiple faces

Forbidden objects

ğŸ”Š Audio Integrity Score (0â€“100)

Based on WavLM similarity:

âœ” High similarity â†’ same speaker

âŒ Sudden drops â†’ possible speaker change

â— speaker_change_flag = True â†’ penalty applied

â­ Overall Score
overall_score = 0.7 * video_score + 0.3 * audio_score

ğŸ§  Models Used
Task	Model	Framework
Face Detection	MTCNN	facenet-pytorch
Eye/Gaze Tracking	FaceMesh	MediaPipe
Object Detection	YOLOv8n	Ultralytics
Speaker Embeddings	WavLM-Base+	HuggingFace
Audio Extraction	ffmpeg	subprocess
ğŸ§© Configuration

Modify detection behavior via:

src/config.yaml


Example:

detection:
  face:
    detection_interval: 5
    min_confidence: 0.8
  eyes:
    gaze_threshold: 2
    blink_threshold: 0.3
  objects:
    min_confidence: 0.65
    max_fps: 5

audio_monitoring:
  sample_rate: 16000

ğŸ§¼ Code Quality Enhancements

Unified scoring pipeline

Improved JSON schemas

Robust Jinja templates

Optimized YOLO inference

Modularized audio engine

Cleaner folder structure

Single unified conda environment

Fixed transformers & tokenizers conflicts

ğŸ›¡ï¸ Privacy Guarantee

âœ” No cloud upload
âœ” No logging of raw video/audio
âœ” 100% offline processing
âœ” Suitable for exams, interviews, assessments

ğŸ“¬ Future Enhancements

Real-time detection (live webcam)

OCR for reading notes on desk

Emotion recognition

Speaker diarization improvements

Deepfake voice detection

GPU-accelerated cloud API (FastAPI)
