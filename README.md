ğŸ¥ğŸ” AI-Powered Exam & Interview Integrity Detection System

Offline Analysis Â· Audio + Video Integrity Â· Object/Gaze/Face Detection Â· Speaker Consistency

This project is an AI-driven integrity analysis system that processes pre-recorded video + audio to detect suspicious behavior during online exams or interview sessions.
It generates a rich, visual Session Integrity Report with:

ğŸ§‘â€ğŸ’» Face presence tracking

ğŸ‘€ Eye/gaze direction analysis

ğŸ“±ğŸ“š Forbidden object detection (phone, book, etc.)

ğŸ—£ï¸ Audio speaker-consistency scoring (WavLM model)

ğŸ“Š Timeline charts

â­ Overall integrity score (0â€“100)

All analysis runs offline on the client machine â€” no data leaves the system.

ğŸš€ Features
ğŸï¸ Video Analysis

Face detection using MTCNN (facenet-pytorch)

Eye tracking using MediaPipe FaceMesh

Gaze direction classification (left, right, center, up, down)

Blink & EAR (Eye Aspect Ratio) tracking

Face-missing alerts (user away from screen)

Multi-face detection (extra persons in frame)

ğŸ“¦ Object Detection

YOLOv8-Nano (lightweight Ultralytics model)

Detects:

Mobile phones

Books / notes

(Easily extendable to more objects)

FPS-aware throttling for real-time efficiency

ğŸ”Š Audio Integrity Analysis

Extracts audio using ffmpeg

Uses WavLM-Base+ Speaker Verification Model

Splits audio into 2â€“3 second chunks

Computes embeddings for every chunk

Measures:

Average similarity

Minimum chunk similarity

Speaker change likelihood

Overall speaker consistency score (0â€“1 â†’ 0â€“100)

ğŸ“‘ Session Integrity Report

Alerts summary (by type, by minute)

Timeline visualization using Chart.js

Speaker consistency visualization

Video score, audio score, and combined score

Stored as JSON per session

Rendered on a Flask dashboard

ğŸ§± Project Architecture
exam-cheating-detection-main/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dashboard/           # Flask web UI
â”‚   â”‚   â”œâ”€â”€ app.py           # Upload, routing, report pages
â”‚   â”‚   â””â”€â”€ templates/       # dashboard.html, upload.html, session_report.html
â”‚   â”‚
â”‚   â”œâ”€â”€ detection/           # All detection modules
â”‚   â”‚   â”œâ”€â”€ face_detection.py
â”‚   â”‚   â”œâ”€â”€ eye_tracking.py
â”‚   â”‚   â”œâ”€â”€ object_detection.py
â”‚   â”‚   â””â”€â”€ multi_face.py
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/               # Audio analysis pipeline
â”‚   â”‚   â”œâ”€â”€ speaker_consistency.py
â”‚   â”‚   â””â”€â”€ utils_audio.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/            # Scoring logic
â”‚   â”‚   â”œâ”€â”€ scoring.py       # Audio + video + combined scoring
â”‚   â”‚   â””â”€â”€ report_generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logging.py       # Alert logging
â”‚   â”‚   â”œâ”€â”€ screenshot_utils.py
â”‚   â”‚   â””â”€â”€ timer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ offline_processor.py # Main offline pipeline
â”‚   â””â”€â”€ config.yaml          # All detection parameters
â”‚
â”œâ”€â”€ uploads/                 # Uploaded audio/video files
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ sessions/            # Stored JSON reports
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                # â† You are here

âš™ï¸ Installation
1ï¸âƒ£ Create Conda environment (Python 3.10 is required)
conda create -n interview310 python=3.10
conda activate interview310

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install ffmpeg (required for audio extraction)

Mac:

brew install ffmpeg

ğŸ§ª Running the App

Start the Flask dashboard:

python -m src.dashboard.app


Navigate to:

http://127.0.0.1:5000

ğŸ“¤ Using the System

Upload pre-recorded video

Upload corresponding audio file (same duration recommended)

Click Analyze Recording

Wait for processing (15 min video â‰ˆ 1â€“2 min offline processing)

View detailed Session Integrity Report

ğŸ“Š Scoring System

We compute three scores:

1. ğŸï¸ Video Integrity Score (0â€“100)

Penalizes:

Face missing

Gaze away (left/right/up/down)

Excessive eye movement

Multiple faces

Forbidden object detection

Weighted formula maps alerts/minute â†’ score.

2. ğŸ”Š Audio Integrity Score (0â€“100)

Computed using WavLM embeddings:

High average similarity â†’ high integrity

Large similarity dips â†’ potential speaker change

speaker_change_flag = True â†’ score penalty

Also robust to:

No audio

Corrupt file

Low activity audio

3. â­ Overall Integrity Score
overall = 0.7 * video_score + 0.3 * audio_score


Weight can be adjusted in scoring.py.

ğŸ§  Models Used
Task	Model	Framework
Face Detection	MTCNN	facenet-pytorch
Eye/Gaze Tracking	FaceMesh	MediaPipe
Object Detection	YOLOv8n	Ultralytics
Speaker Embeddings	WavLM-Base+	HuggingFace Transformers
Audio Extraction	ffmpeg	subprocess

All models are pre-trained, so no fine-tuning needed and processing is efficient.

ğŸ§© Configuration

Modify detection parameters in:

src/config.yaml


Examples:

detection:
  face:
    detection_interval: 5
    min_confidence: 0.8
  eyes:
    gaze_threshold: 2
    blink_threshold: 0.3
  objects:
    min_confidence: 0.65
    max_fps: 5
audio_monitoring:
  sample_rate: 16000

ğŸ§¼ Code Quality Improvements Implemented

Unified scoring pipeline

Defensive JSON schema handling

Robust Jinja templating (handling dict/float audio)

Optimized YOLO inference (resize + FPS throttle)

Easier debugging (python -m src.dashboard.app)

Fully isolated audio module (audio/speaker_consistency.py)

Environment fixes (transformers + tokenizers compatibility)

Removed .venv conflicts in favor of single conda environment

ğŸ›¡ï¸ Privacy & Local-Only Guarantee

This system performs all processing offline.
No recordings, audio, or metadata is uploaded to any external server.

ğŸ“¬ Future Enhancements

Real-time detection mode

Emotion detection

OCR-based note detection

Better multi-speaker diarization

Deepfake voice detection

Cloud deployment (FastAPI + GPU support)